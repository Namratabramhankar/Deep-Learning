{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PACKAGES\n",
    "1) numpy is the fundamental package for scientific computing with Python.\n",
    "2) h5py is a common package to interact with a dataset that is stored on an H5 file.\n",
    "3) matplotlib is a famous library to plot graphs in Python.\n",
    "4) PIL and scipy are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. OVERVIEW OF THE PROBLEM SET\n",
    "Problem Statement: You are given a dataset (\"data.h5\") containing:\n",
    "\n",
    " a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    " a test set of m_test images labeled as cat or non-cat\n",
    " each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Find the values for:\n",
    "\n",
    "- m_train (number of training examples)\n",
    "- m_test (number of test examples)\n",
    "- num_px (= height = width of a training image)\n",
    "Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px  ∗∗  num_px  ∗∗  3, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will build a Logistic Regression, using a Neural Network mindset.\n",
    "The main steps for building a Neural Network are:\n",
    "\n",
    "-Define the model structure (such as number of input features)\n",
    "-Initialize the model's parameters\n",
    "-Loop:\n",
    "    Calculate current loss (forward propagation)\n",
    "    Calculate current gradient (backward propagation)\n",
    "    Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement sigmoid(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = np.zeros(shape=(dim,1),dtype=float)\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement a function propagate() that computes the cost function and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)                                    # compute activation\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))                                 # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = (1/m)*np.dot(X,(A-Y).T)\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Write down the optimization function. The goal is to learn  w  and  b  by minimizing the cost function  J . For a parameter  θ , the update rule is  θ=θ−αdθ, where α is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.There are two steps to computing predictions:\n",
    "\n",
    "Calculate  Ŷ =A=σ(wTX+b)\n",
    "Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[:,i]<0.5:\n",
    "            Y_prediction[:,i] = 0\n",
    "        else:\n",
    "            Y_prediction[:,i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement the model function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxV9Z3/8dcnCUlIQjYSICQhYRVFQCWAK25d0NpaqzhutdV2HNphOp12FmfmN51OZzqPaTvtjK3tWGvVtlq3apVSl7pVXKoSICCrRNawJgQIhCWEfH5/nBO8xpuQkNzcJPf9fDzuI/ee873nfu7hct/3bN+vuTsiIpK4kuJdgIiIxJeCQEQkwSkIREQSnIJARCTBKQhERBKcgkBEJMEpCGRAMrNnzOxz8a5DpD9QEEiPMrONZvaReNfh7pe5+y/iXQeAmf3RzL7YC6+TZmb3mlmDme0ws6+doP0NZrbJzBrN7Ekzy+/ssszMw+cdCG/3xOp9SewpCKTfMbOUeNfQqi/VAnwTGA+UARcDf29ms6M1NLNJwE+BzwLDgYPAT7q4rKnunhXeYh50EjsKAuk1ZnaFmVWZ2V4ze8PMpkTMu93M3jOz/Wa2ysyuipj3eTN73cz+x8zqgW+G014zs/82sz1mtsHMLot4zvFf4Z1oO9rMFoav/YKZ/djMHmjnPVxkZjVm9g9mtgO4z8zyzGyBmdWGy19gZiVh+28DFwB3hr+c7wynTzSz582s3szWmtm1PbCKbwb+3d33uPtq4GfA59tpeyPwO3df6O4HgH8BPmNmQ05iWdLPKQikV5jZWcC9wF8AQwl+jc43s7SwyXsEX5g5wL8BD5hZUcQiZgLrgWHAtyOmrQUKgO8CPzcza6eEjtr+Gng7rOubBL+SOzICyCf4tXwbwf+j+8LHo4BDwJ0A7v7PwKvAvPCX8zwzywSeD193GHA98JPwV/qHmNlPwvCMdlsetskDRgLLIp66DIi6zHD68bbu/h7QBEzowrIWhruNnjCz8nZeR/oBBYH0lj8Hfurub7n7sXD//RHgbAB3f8zdt7l7i7s/AqwDZkQ8f5u7/8jdm939UDhtk7v/zN2PAb8Aigh2c0QTta2ZjQKmA99w9yZ3fw2Yf4L30gL8q7sfcfdD7r7b3R9394Puvp8gqC7s4PlXABvd/b7w/SwBHgeuidbY3b/s7rnt3Fq3qrLCv/sinroPGEJ0WW3aRrbvzLIuBMqBicA2YEEf200mXaAgkN5SBnw98tcsUErwyxMzuzlit9Fe4HSCX++ttkRZ5o7WO+5+MLybFaVdR21HAvUR09p7rUi17n649YGZZZjZT8MDrw3AQiDXzJLbeX4ZMLPNuriRYEvjZB0I/2ZHTMsG9nfQPrvNtNb2J1xWuEupyd33An8NjAZOPbnSJd4UBNJbtgDfbvNrNsPdHzKzMoJ90POAoe6eC6wAInfzxKqb3O1AvpllREwrPcFz2tbydeAUYKa7ZwOzwunWTvstwCtt1kWWu38p2ouZ2V0RZ+e0va0EcPc94XuZGvHUqcDKdt7Dysi2ZjYGSAPePYlltb7H9nbLSR+nIJBYGGRm6RG3FIIv+rlmNtMCmWb2ifDgZCbBF0ktgJndQrBFEHPuvgmoJDgAnWpm5wCf7OJihhAcF9hrwSmY/9pm/k5gTMTjBQT74j9rZoPC23Qzi/qL2t3nRpyd0/YWud/+l8D/Cw9eTyTYHXd/OzU/CHzSzC4Ij1l8C3gi3LXV4bLMbJKZnWFmyWaWBXwf2AqsPvGqkr5IQSCx8DTBF2Pr7ZvuXknwZXInsAeoJjwLxd1XEXyZ/IngS3My8Hov1nsjcA6wG/gP4BGC4xed9b/AYKAOeBN4ts38O4BrwjOKfhh+2X4MuI5g//oO4DsEv8i7418JDrpvAl4Bvufux2sJtyAuAHD3lcBcgkDYRRBmX+7ksoYTrKMGggP45cAV7n60m/VLnJgGphH5IDN7BFjj7m1/2YsMSNoikIQX7pYZa2ZJFlw0dSXwZLzrEuktOt1LJDhb5wmC6whqgC+5+9L4liTSe7RrSEQkwWnXkIhIgut3u4YKCgq8vLw83mWIiPQrixcvrnP3wmjz+l0QlJeXU1lZGe8yRET6FTPb1N487RoSEUlwCgIRkQSnIBARSXAxDQIzmx0OulFtZrdHmf93YY+TVWa2wsyOWcRweSIiEnsxC4KwC94fA5cBpwHXm9lpkW3c/Xvufoa7nwH8I0GPjPWxqklERD4sllsEM4Bqd1/v7k3AwwSX7rfneuChGNYjIiJRxDIIivngAB814bQPCfuCn00wSlO0+beZWaWZVdbW1vZ4oSIiiSyWQRBtkIr2+rP4JPB6e7uF3P1ud69w94rCwqjXQ5xQ9a4DfOt3qzh6rOWkni8iMlDFMghq+OBITyUEfa9Hcx0x3i20ub6Re1/fwB9W7ozly4iI9DuxDIJFwHgzG21mqQRf9h8aFNzMcggGwn4qhrVw4YRhlOQN5oE32724TkQkIcUsCNy9mWAM2ucIhrB71N1XmtlcM5sb0fQq4A/u3hirWgCSk4wbZo7iT+t3U72rvfG8RUQST0yvI3D3p919gruPdfdvh9Pucve7Itrc7+7XxbKOVtdWlJKanMQDb27ujZcTEekXEurK4oKsNC6bPILHF9dwsKk53uWIiPQJCRUEAJ89u4z9R5qZX9XecWsRkcSScEEwrSyPiSOG8Ks3N6HR2UREEjAIzIwbzy5j5bYGqrbsjXc5IiJxl3BBAHDVmcVkpibzK51KKiKSmEGQlZbCVWcVs2D5dvY0NsW7HBGRuErIIAC46ewymppbeGzxlhM3FhEZwBI2CCaOyGZ6eR4PvrWZlhYdNBaRxJWwQQDBVsGm3Qd5tbou3qWIiMRNQgfB7NNHMDQzVf0PiUhCS+ggSEtJ5s+ml/Li6p1s23so3uWIiMRFQgcBwPUzRuHAQ2+r/yERSUwJHwSl+RlccsowHl60haZmDVojIokn4YMAgoPGtfuP8IdVO+JdiohIr1MQALMmFFKar0FrRCQxKQgIB62ZUcab6+tZt1OD1ohIYlEQhK6tKCE1OYkH39JBYxFJLAqC0NCsNC4PB61pPKJBa0QkcSgIItzUOmjNMg1aIyKJQ0EQ4figNX/SoDUikjgUBBHMjJvOLmPV9gaWatAaEUkQCoI2Pn1mMVlpKTzwJ51KKiKJQUHQRlZaCledWcyCd7ZTr0FrRCQBKAiiOD5oTaUGrRGRgU9BEMUpI4YwozyfX7+tQWtEZOBTELTjpnM0aI2IJIaYBoGZzTaztWZWbWa3t9PmIjOrMrOVZvZKLOvpitmTRlCQlcqvdNBYRAa4mAWBmSUDPwYuA04Drjez09q0yQV+AnzK3ScBc2JVT1elpiRxbUUpL63ZyVYNWiMiA1gstwhmANXuvt7dm4CHgSvbtLkBeMLdNwO4+64Y1tNlN8wMB61R/0MiMoDFMgiKgcjTbmrCaZEmAHlm9kczW2xmN8ewni4rydOgNSIy8MUyCCzKtLan4KQA04BPAB8H/sXMJnxoQWa3mVmlmVXW1tb2fKUduOmcMuoOHOG5lRq0RkQGplgGQQ1QGvG4BGjbm1sN8Ky7N7p7HbAQmNp2Qe5+t7tXuHtFYWFhzAqO5sLxGrRGRAa2WAbBImC8mY02s1TgOmB+mzZPAReYWYqZZQAzgdUxrKnLkpKMG2eW8daGet7VoDUiMgDFLAjcvRmYBzxH8OX+qLuvNLO5ZjY3bLMaeBZYDrwN3OPuK2JV08maMy0ctEZbBSIyAFl/6265oqLCKysre/11/+aRKp5ftZO3/ulSMtNSev31RUS6w8wWu3tFtHm6sriTbpw5igNHmvn9O9vjXYqISI9SEHTStLI8xhRk8pvKmniXIiLSoxQEnWRmXFNRwtsb69lQ1xjvckREeoyCoAuuPquEJIPfLFb31CIycCgIumB4djoXTijk8cVbOabuqUVkgFAQdNG1FaXsaDjMq+t69wpnEZFYURB00aWnDicvYxCP6aCxiAwQCoIuSk1J4tNnFvP8qp3s0ZjGIjIAKAhOwpxppTQda+Gpqq3xLkVEpNsUBCfhtJHZnF6czWOLtXtIRPo/BcFJmjOtlJXbGli5bV+8SxER6RYFwUm68oyRpCYn6aCxiPR7CoKTlJuRykcnDefJqq0caT4W73JERE6agqAb5kwrYe/Bo7y4uk8NtSwi0iUKgm64YHwhI7LTebRSXU6ISP+lIOiG5CTj6mnFLHy3lh37Dse7HBGRk6Ig6KY500ppcXhiqQ4ai0j/pCDopvKCTGaU5/NYZQ39bbQ3ERFQEPSIORUlbKhrZPGmPfEuRUSkyxQEPeDyyUVkpCbroLGI9EsKgh6QmZbCFVOK+P3y7TQeaY53OSIiXaIg6CFzKkppbDrG0xrcXkT6GQVBD6koy2N0QaY6ohORfkdB0EPMjGumlfD2hno2anB7EelHFAQ96P3B7bVVICL9h4KgB43ISWfWhEJ+s7hGg9uLSL+hIOhhc6YFg9u/Vl0X71JERDolpkFgZrPNbK2ZVZvZ7VHmX2Rm+8ysKrx9I5b19IaPnDaM3IxBuqZARPqNlFgt2MySgR8DHwVqgEVmNt/dV7Vp+qq7XxGrOnpbWkoynz6jmF+/tZm9B5vIzUiNd0kiIh2K5RbBDKDa3de7exPwMHBlDF+vz5hTURIObr8t3qWIiJxQLIOgGIjcP1ITTmvrHDNbZmbPmNmkaAsys9vMrNLMKmtra2NRa4+aNDKH04qyeWyxdg+JSN8XyyCwKNPankqzBChz96nAj4Anoy3I3e929wp3rygsLOzhMmPj2ooSVmxtYNW2hniXIiLSoVgGQQ1QGvG4BPjAvhJ3b3D3A+H9p4FBZlYQw5p6zZVnFAeD22urQET6uFgGwSJgvJmNNrNU4DpgfmQDMxthZhbenxHWszuGNfWavMxUPnracJ5cupWm5pZ4lyMi0q6YBYG7NwPzgOeA1cCj7r7SzOaa2dyw2TXACjNbBvwQuM4H0Ogu11SUsOfgUV5cvTPepYiItCtmp4/C8d09T7eZdlfE/TuBO2NZQzzNCge3f2xxDZdNLop3OSIiUenK4hhKTjI+c1Yxf1y7i50NGtxeRPomBUGMzakIB7dfsjXepYiIRKUgiLHRBZlML8/jscotGtxeRPokBUEvmFNRyvq6RpZs1uD2ItL3KAh6wSdaB7dfpHEKRKTvURD0gsy0FC6fXMSC5ds42KTB7UWkb1EQ9JLrZ4yisekYD7+tK41FpG9REPSSaWV5nDNmKHe98h6Hjx6LdzkiIscpCHrRVy4dz679RzRojYj0KQqCXnT2mHxmlOfzf398jyPN2ioQkb5BQdCLzIyvXDqe7fsO85vFOoNIRPoGBUEvO2/cUM4alctPXn5PvZKKSJ+gIOhlrVsFW/ce4rdLtVUgIvGnIIiDCycUMrUkhztfruboMW0ViEh8KQjioHWrYEv9IQ1wLyJxpyCIk0smDmPSyGx+/HI1zdoqEJE46lQQmNmczkyTzmvdKthQ18iC5dvjXY6IJLDObhH8YyenSRd89NThTBwxhB+9tI5jLeqiWkTio8OhKs3sMuByoNjMfhgxKxtQ72ndlJQUbBV8+cElPP3Odj45dWS8SxKRBHSiLYJtQCVwGFgccZsPfDy2pSWG2ZNGMH5YFj96aR0t2ioQkTjoMAjcfZm7/wIY5+6/CO/PB6rdXaOs9ICkJGPeJeN4d+cBnlu5I97liEgC6uwxgufNLNvM8oFlwH1m9oMY1pVQrpgykjEFmdzxorYKRKT3dTYIcty9AfgMcJ+7TwM+EruyEktyuFWwZsd+Xli9M97liEiC6WwQpJhZEXAtsCCG9SSsT00dSdnQDH740joNci8ivaqzQfAt4DngPXdfZGZjgHWxKyvxpCQn8ZcXj2PF1gZeXrsr3uWISALpVBC4+2PuPsXdvxQ+Xu/uV8e2tMRz1ZnFlOQN5o4Xq7VVICK9prNXFpeY2W/NbJeZ7TSzx82sJNbFJZpB4VbBsi17WbiuLt7liEiC6OyuofsIThsdCRQDvwundcjMZpvZWjOrNrPbO2g33cyOmdk1naxnwLr6rBJG5qRzxwvvaqtARHpFZ4Og0N3vc/fm8HY/UNjRE8wsGfgxcBlwGnC9mZ3WTrvvEByDSHipKUl86eJxLNm8lzfe2x3vckQkAXQ2COrM7CYzSw5vNwEn+paaQXDh2Xp3bwIeBq6M0u6vgMcBHSENXVtRwojsdO54UcfjRST2OhsEtxKcOroD2A5cA9xygucUA1siHteE044zs2LgKuCujhZkZreZWaWZVdbW1nay5P4rLSWZuReO4e0N9by5XlsFIhJbnQ2Cfwc+5+6F7j6MIBi+eYLnWJRpbXd6/y/wD+5+rKMFufvd7l7h7hWFhR3ukRowrpsxisIhafxQWwUiEmOdDYIpkX0LuXs9cOYJnlMDlEY8LiHoxC5SBfCwmW0k2Mr4iZl9upM1DWjpg5L5i1ljeOO93SzaWB/vckRkAOtsECSZWV7rg7DPoQ67sAYWAePNbLSZpQLXEZx5dJy7j3b3cncvB34DfNndn+x09QPcjTPLKMhK1VaBiMRUZ4Pg+8AbZvbvZvYt4A3gux09wd2bgXkEZwOtBh5195VmNtfM5nan6EQxODWZP79gDK+uq2PJZnX2KiKxYZ09Vz089fMSgn3/L7r7qlgW1p6KigqvrKyMx0vHReORZs7/zkucUZrLfbfMiHc5ItJPmdlid6+INu9Eu3eOC7/44/Lln8gy01L44gVj+N5za1les5cpJbnxLklEBpjO7hqSOLr5nDJyBg/ihy9Wx7sUERmAFAT9wJD0QXzh/NG8sHqnRjETkR6nIOgnvnD+aKaW5jLv10sUBiLSoxQE/URmWgq/+sIMJo3M4S8fXMKzKxQGItIzFAT9SHb6IH75hRlMLslh3q+X8OyK7fEuSUQGAAVBP5OdPohf3toaBkt55h2FgYh0j4KgHxoShsGUkhzmPaQwEJHuURD0U0PSB/GLW2dwRmku8x5ayu+XKwxE5OQoCPqx1jA4szSXrzy8lAXL2/bpJyJyYgqCfi4rLYX7b53BWaNy+euHq/jdMoWBiHSNgmAAyEpL4b5bgjD46iMKAxHpGgXBAJGVlsL9t8xg2qg8/vrhpcxXGIhIJykIBpDMtBTuu2U6FeX5fPXhpTxVtTXeJYlIP6AgGGAy01K4/5bpTC/P528eqeLJpQoDEemYgmAAykgNtgxmjM7na49W8dulNfEuSUT6MAXBAJWRmsK9n5/OzNFD+fqjyxQGItIuBcEA1hoGZ48ZytceXcYTSxQGIvJhCoIBbnBqMj//3HTOHTuUrz+2jG/9bhWNR5rjXZaI9CEKggQwODWZe26ezg0zRnHv6xv42P8s5MXVO+Ndloj0EQqCBDE4NZlvXzWZx790DplpyXzhF5V8+cHF7Go4HO/SRCTOFAQJZlpZPgv+6gL+9mMTeGH1Li79/is88OYmWlo83qWJSJwoCBJQakoS8y4Zz3NfncXkkhz+35MrmPPTP/Huzv3xLk1E4kBBkMBGF2Ty4Bdn8v05U1lfe4BP/PBV/vu5tRw+eizepYlIL1IQJDgz4+ppJbz49Yv45NSR3PlyNbP/dyFvVNfFuzQR6SUKAgEgPzOVH1x7Bg9+cSYAN9zzFl97tIr6xqY4VyYisaYgkA84b1wBz351Fn958VjmV23j0u//kccX1+Cug8kiA1VMg8DMZpvZWjOrNrPbo8y/0syWm1mVmVWa2fmxrEc6J31QMn/38Yn8/isXMLogk68/toybfv4WG+oa412aiMSAxeqXnpklA+8CHwVqgEXA9e6+KqJNFtDo7m5mU4BH3X1iR8utqKjwysrKmNQsH9bS4vz67c1855k1HDnWwufPLWfuhWPJz0yNd2ki0gVmttjdK6LNi+UWwQyg2t3Xu3sT8DBwZWQDdz/g7ydRJqD9D31MUpJx09llvPD1C7liShE/e3U9s777Mv/z/LvsP3w03uWJSA+IZRAUA1siHteE0z7AzK4yszXA74Fboy3IzG4Ldx1V1tbWxqRY6djw7HR+cO0ZPPfVWZw/roA7XlzHrO++zN0L39PppiL9XCyDwKJM+9Avfnf/bbg76NPAv0dbkLvf7e4V7l5RWFjYw2VKV0wYPoS7PjuN+fPOY3JJLv/59Bou/N7L/OrNTTQ1t8S7PBE5CbEMghqgNOJxCdDuQLruvhAYa2YFMaxJesiUklx+eesMHr7tbErzMviXJ1dw6Q/+yBNLajim7ipE+pVYBsEiYLyZjTazVOA6YH5kAzMbZ2YW3j8LSAV2x7Am6WFnjxnKY3PP4b7PT2dI2iC+9ugyZv/vQp5dsV2nnIr0EymxWrC7N5vZPOA5IBm4191XmtnccP5dwNXAzWZ2FDgE/Jnr26PfMTMunjiMCycU8syKHXz/+bXMfWAJU0py+NuPncIF4wsI815E+qCYnT4aKzp9tO9rPtbCE0u3cscL69i69xAzR+fzdx8/hYry/HiXJpKwOjp9VEEgMXOk+RgPvbWZO1+upu5AExefUsiXLx5HRVmethBEepmCQOLqYFMz97+xkZ++sp59h44ytSSHW88fzeWTixiUrF5ORHqDgkD6hINNzTy+uIZ7X9/IhrpGinLS+dy55Vw/fRQ5GYPiXZ7IgKYgkD6lpcV5ee0u7nl1A39av5uM1GTmTCvhlvNGU16QGe/yRAYkBYH0WSu37ePnr23gd8u20dzifOTU4Xzx/NHMGJ2v4wgiPUhBIH3erobD/PJPm3jgrU3sPXiU04uz+eL5Y7h8chGpKTqOINJdCgLpNw41HeOJpTXc+9oG3qttZHh2Gp87t5wbZowiN0M9noqcLAWB9DstLc4r79by89c28Fp1HYMHJXP1tGJunFnGqUXZ8S5PpN9REEi/tnp7A/e+toGnqrbRdKyFycU5XFtRwqemFutsI5FOUhDIgFDf2MRTVVt5tLKG1dsbSE1J4uOTRnBtRQnnjS0gKUkHl0XaoyCQAWfF1n08VrmFJ6u2se/QUYpzB3P1tBLmTCuhND8j3uWJ9DkKAhmwDh89xgurd/JoZQ2vrqvFHc4ZM5Rrp5cwe1IRg1OT412iSJ+gIJCEsG3vIR5fXMNji2vYXH+QIWkpXDF1JNdWlHBGaa6uS5CEpiCQhNLS4ry9sZ5HK7fw9DvbOXy0hfHDspgTHmAekZMe7xJFep2CQBLW/sNHWbB8O49WbmHp5r0AVJTlcfnkIi6fXKRQkIShIBAB3qs9wNPLt/P7d7azZsd+IAiFT0wp4rLTFQoysCkIRNqIFgrTy4MtBYWCDEQKApEOtA0Fsw/uPhqerVCQ/k9BINJJ1bsO8PQ723m6TSh8YnIRlykUpB9TEIichGihMKUkl0tOGcalpw5j0shsnZIq/YaCQKSbqncd4NkV23lxzS6qtuzFHYYNSeOSicO4eOIwzh9XQGZaSrzLFGmXgkCkB9UdOMIra2t5ac0uFr5by/4jzaQmJzFzTD6XTBzGJROHUTZUI61J36IgEImRo8daWLSxnpfX7OKlNbt4r7YRgLGFmWEoDKeiPI9ByRpcR+JLQSDSSzbtbuSlMBTeWl9P07EWhqSnMGt8IRdPHMas8QUM0wFniQMFgUgcHDjSzGvr6oKthbW7qN1/BIDxw7I4b1wB544dytljh5KdrjEVJPYUBCJx1tLirNrewOvVdbz+3m4Wbajn0NFjJBlMLsnlvLFDOW9cAdPK8kgfpB5TpecpCET6mCPNx1i6eS9vhMFQtWUvx1qc1JQkppfnce7YAs4bV8Dk4hySNeCO9IC4BYGZzQbuAJKBe9z9v9rMvxH4h/DhAeBL7r6so2UqCGQgOnCkmbc37Ob16t28Xl13vNuLIekpnD1m6PEthnHDsnTtgpyUjoIgZic+m1ky8GPgo0ANsMjM5rv7qohmG4AL3X2PmV0G3A3MjFVNIn1VVloKl0wcziUThwPBKapvvLc73GKo4/lVOwEYmplKRXke08vzqSjPZ9LIbJ2RJN0WyytgZgDV7r4ewMweBq4EjgeBu78R0f5NoCSG9Yj0GwVZaXxq6kg+NXUkAFvqD/J6dR2LNu6hclM9z60MgmHwoGTOHJVLRXk+08vzOGtUni5sky6L5SemGNgS8biGjn/tfwF4JtoMM7sNuA1g1KhRPVWfSL9Rmp/BdTNGcd2M4PO/s+EwlRv3sGhjPZWb6rnzpXW0OCQnGacVZUdsNeQxbIhOV5WOxTIIou3IjHpAwswuJgiC86PNd/e7CXYbUVFR0b+ObovEwPDsdD4xpYhPTCkCggF4lm7eS+XGehZt3MNDb2/mvtc3AlA+NIOK8nxmlOdz5qhcxhZmkaQD0BIhlkFQA5RGPC4BtrVtZGZTgHuAy9x9dwzrERmwhqQPYtaEQmZNKASCK55XbN13fKvhpTW7+M3iGiA4HjG5OIczRuUytSSXM0flqlfVBBezs4bMLAV4F7gU2AosAm5w95URbUYBLwE3tzle0C6dNSTSde7O+rpGqjbvpWrLXpbV7GX19gaOHgv+/4/ITmdqaQ5nlOYxtTSHKSW5ZOlYw4ASl7OG3L3ZzOYBzxGcPnqvu680s7nh/LuAbwBDgZ+Ep8Q1t1eoiJw8M2NsYRZjC7O4elpwTsbho8dYtb2BZVvCcNiy9/hBaLPgCuipJblMLc3ljNJcThkxRGcoDVC6oExEjtvT2MSymveDoWrLXvYcPApAWkoSpxZlM2lkNqcX5zBpZDYThg/RldD9hK4sFpGT4u5sqT9EVU0QDCu27mPVtgb2H2kGICXJGDcs63gwnF6cw6lF2dqt1AcpCESkx7S0OFv2HGTltgZWbN3Hym0NrNy2j7oDTUCwW6l8aCaTRmYzaWQOpxcHf/MzU+NceWKLyzECERmYkpKMsqGZlA3N5PLJwemr7s6u/UdYuW0fK7c2sGLbPqq27GXB8u3Hn1eUk86pRdmcMmIIE0cM4ZQRQxhTkEVqio47xJuCQES6zcwYnp3O8Oz0491kAOw92MSqbQ3HtxrW7NjPq+tqj5+tlJIUHMQ+JQyG1oAozh2sPpV6kb0VISUAAAwsSURBVIJARGImNyOVc8cVcO64guPTmppb2FDXyJodDazdsZ+1O/azeNMe5i97/zKjIWkpTIgMh+FDmDgim5wMjd0QCwoCEelVqSlJx7cAIjUcPsq7O/azJgyHtTv2s2DZNn79VvPxNsOGpDFuWBbjh2UxblgWY8O/hVlp2oLoBgWBiPQJ2emDqAh7VW3l7uxoOHw8HNbtPEB17QEeX7KVA0feD4icwYMYNyyLcYVBMIwbHtwvzh2s7jQ6QUEgIn2WmVGUM5iinMFcfMqw49NbA6J614Hjt3W7DvDC6p08Uvl+X5eDByUzdljm8YAYW5jF6MJMyodm6vqHCAoCEel3IgPigvGFH5i3p7GJ6toDwdbDrmALYtHGPTxZ9cGuzopzBzO6IPP9W2Emo4dmUpI3mJQEu4JaQSAiA0peZirTM/OZHrGLCaDxSDMbdzeyoa6RDbXB3/V1jTxVtZWGw+/vZhqUbJTmZzDmeEhkMbogkzGFmQwbMjCPRSgIRCQhZKalMGlkDpNG5nxguruz5+BRNtQdYH0YEK23V9fVcaS55XjbwYOSGZWfwaihGZTlZ1A2NINRQzMpy8+gOG9wv+2LSUEgIgnNzMjPTCU/M59pZR/cimhpcbY3HGZDbSPr6w6waffB8NbIq+tqOXz0/ZBITjJG5qZTlp/5waDIz6RsaEafHjmu71YmIhJnSUlGce5ginMHc/74gg/Ma72aujUYNteHIVF/kGfe2X68s75WBVmplOZnUJKXQWne4OBvfvB3ZG46aSnxO3itIBAROQmRV1PPGJ3/ofkNh4+yuXULor6RzbsPsmXPQZbX7OWZd7bT3OIRy4LhQ9IpyRschsXg4H5eEBxFuekx3e2kIBARiYHs9EGcXpzD6cU5H5p3rMXZ2XCYLfUHqdlziC17gr81ew7y9oZ6nqo6REROkGRQlDOYz59bzp/PGtPjtSoIRER6WXA8YTAjcwczM8r8o8da2LHvcBAQ9UFAbNlziGHZaTGpR0EgItLHDEpOojQ/g9L8DBgb+9frn+c6iYhIj1EQiIgkOAWBiEiCUxCIiCQ4BYGISIJTEIiIJDgFgYhIglMQiIgkOHP3E7fqQ8ysFth0kk8vAOp6sJye1tfrg75fo+rrHtXXPX25vjJ3L4w2o98FQXeYWaW7V8S7jvb09fqg79eo+rpH9XVPX6+vPdo1JCKS4BQEIiIJLtGC4O54F3ACfb0+6Ps1qr7uUX3d09friyqhjhGIiMiHJdoWgYiItKEgEBFJcAMyCMxstpmtNbNqM7s9ynwzsx+G85eb2Vm9WFupmb1sZqvNbKWZ/XWUNheZ2T4zqwpv3+it+sLX32hm74SvXRllfjzX3ykR66XKzBrM7Ktt2vT6+jOze81sl5mtiJiWb2bPm9m68G9eO8/t8PMaw/q+Z2Zrwn/D35pZbjvP7fDzEMP6vmlmWyP+HS9v57nxWn+PRNS20cyq2nluzNdft7n7gLoBycB7wBggFVgGnNamzeXAM4ABZwNv9WJ9RcBZ4f0hwLtR6rsIWBDHdbgRKOhgftzWX5R/6x0EF8rEdf0Bs4CzgBUR074L3B7evx34TjvvocPPawzr+xiQEt7/TrT6OvN5iGF93wT+thOfgbisvzbzvw98I17rr7u3gbhFMAOodvf17t4EPAxc2abNlcAvPfAmkGtmRb1RnLtvd/cl4f39wGqguDdeuwfFbf21cSnwnruf7JXmPcbdFwL1bSZfCfwivP8L4NNRntqZz2tM6nP3P7h7c/jwTaCkp1+3s9pZf50Rt/XXyswMuBZ4qKdft7cMxCAoBrZEPK7hw1+0nWkTc2ZWDpwJvBVl9jlmtszMnjGzSb1aGDjwBzNbbGa3RZnfJ9YfcB3t/+eL5/prNdzdt0PwAwAYFqVNX1mXtxJs5UVzos9DLM0Ld13d286utb6w/i4Adrr7unbmx3P9dcpADAKLMq3tObKdaRNTZpYFPA581d0b2sxeQrC7YyrwI+DJ3qwNOM/dzwIuA/7SzGa1md8X1l8q8CngsSiz473+uqIvrMt/BpqBB9tpcqLPQ6z8H8HQ7WcA2wl2v7QV9/UHXE/HWwPxWn+dNhCDoAYojXhcAmw7iTYxY2aDCELgQXd/ou18d29w9wPh/aeBQWZW0Fv1ufu28O8u4LcEm9+R4rr+QpcBS9x9Z9sZ8V5/EXa27jIL/+6K0iben8XPAVcAN3q4Q7utTnweYsLdd7r7MXdvAX7WzuvGe/2lAJ8BHmmvTbzWX1cMxCBYBIw3s9Hhr8brgPlt2swHbg7Pfjkb2Ne6CR9r4f7EnwOr3f0H7bQZEbbDzGYQ/Dvt7qX6Ms1sSOt9ggOKK9o0i9v6i9Dur7B4rr825gOfC+9/DngqSpvOfF5jwsxmA/8AfMrdD7bTpjOfh1jVF3nc6ap2Xjdu6y/0EWCNu9dEmxnP9dcl8T5aHYsbwVkt7xKcTfDP4bS5wNzwvgE/Due/A1T0Ym3nE2y6LgeqwtvlbeqbB6wkOAPiTeDcXqxvTPi6y8Ia+tT6C18/g+CLPSdiWlzXH0EobQeOEvxK/QIwFHgRWBf+zQ/bjgSe7ujz2kv1VRPsX2/9HN7Vtr72Pg+9VN+vws/XcoIv96K+tP7C6fe3fu4i2vb6+uvuTV1MiIgkuIG4a0hERLpAQSAikuAUBCIiCU5BICKS4BQEIiIJTkEgMWFmb4R/y83shh5e9j9Fe61YMbNPx6oHUzM7EKPlXmRmC7q5jPvN7JoO5s8zs1u68xrSNygIJCbc/dzwbjnQpSAws+QTNPlAEES8Vqz8PfCT7i6kE+8r5sIrYXvKvcBXenB5EicKAomJiF+6/wVcEPbF/jdmlhz2g78o7EzsL8L2F1kwTsOvCS4iwsyeDDvqWtnaWZeZ/RcwOFzeg5GvFV7p/D0zWxH2//5nEcv+o5n9xoL+9x+MuPL4v8xsVVjLf0d5HxOAI+5eFz6+38zuMrNXzexdM7sinN7p9xXlNb5tQQd5b5rZ8IjXuSaizYGI5bX3XmaH014j6Pag9bnfNLO7zewPwC87qNXM7M5wffyeiE7yoq0nD65G3hhevS39WE/+OhCJ5naCPuVbvzBvI+iSYrqZpQGvh19QEPTBcrq7bwgf3+ru9WY2GFhkZo+7++1mNs/dz4jyWp8h6KBsKlAQPmdhOO9MYBJBPzSvA+eZ2SqCrgsmurtb9IFZziPoxC5SOXAhQYdoL5vZOODmLryvSJnAm+7+z2b2XeDPgf+I0i5StPdSSdAfzyUEVwy37ftmGnC+ux/q4N/gTOAUYDIwHFgF3Gtm+R2sp0qC3jffPkHN0odpi0B628cI+imqIuh+eygwPpz3dpsvy6+YWWs3EaUR7dpzPvCQBx2V7QReAaZHLLvGgw7Mqgi+zBuAw8A9ZvYZIFp/O0VAbZtpj7p7iwfdDq8HJnbxfUVqAlr35S8O6zqRaO9lIrDB3dd50F3AA22eM9/dD4X326t1Fu+vv23AS2H7jtbTLoIuFaQf0xaB9DYD/srdn/vARLOLgMY2jz8CnOPuB83sj0B6J5bdniMR948RjMzVHO7WuJSgs7J5BL+oIx0CctpMa9svi9PJ9xXFUX+/n5djvP9/spnwh1q46ye1o/fSTl2RImtor9bLoy3jBOspnWAdST+mLQKJtf0EQ3K2eg74kgVdcWNmEyzolbGtHGBPGAITCYbEbHW09fltLAT+LNwHXkjwC7fdXRYWjAmR40FX1V8l2K3U1mpgXJtpc8wsyczGEnQqtrYL76uzNhLszoFgxK1o7zfSGmB0WBMEvbO2p71aFwLXheuvCLg4nN/ReppAX+xNU7pEWwQSa8uB5nAXz/3AHQS7MpaEv3RriT6E47PAXDNbTvBF+2bEvLuB5Wa2xN1vjJj+W+Acgp4eHfh7d98RBkk0Q4CnzCyd4Ffy30RpsxD4vplZxC/3tQS7nYYT9Dx52Mzu6eT76qyfhbW9TdBzaUdbFYQ13Ab83szqgNeA09tp3l6tvyX4pf8OQW+er4TtO1pP5wH/1uV3J32Keh8VOQEzuwP4nbu/YGb3Awvc/TdxLivuzOxM4Gvu/tl41yLdo11DIif2nwRjIMgHFQD/Eu8ipPu0RSAikuC0RSAikuAUBCIiCU5BICKS4BQEIiIJTkEgIpLg/j89t/SnkidhBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
